# Default parameter configuration for running Retrieval-Augmented Generation

## Database ##

# Text splitter parameters
chunk_size: 1000
chunk_overlap: 200

## Query ##

# Hugging face instructor embeddings
model_name: "hkunlp/instructor-xl"
model_kwargs:
  "device": "cuda"

# ChromaDB retriever
search_type: "mmr"
#  k is the amount of documents to return (sources/citations)
#  fetch_k is the amount of documents to pass to MMR algorithm
search_kwargs:
  "k": 3
  "fetch_k": 50

# Model
# model_id: "Llama2-7b-chat-hf/"
model_id: "Llama2-13b-chat-hf/"
# model_id: "Llama2-70b-chat-hf/"

# Model token max length
token_max: 4096  # Llama-2

# LM input
load_in_8bit: True
lm_device_map: "auto"

# Prompt pipeline (summary)
pipeline_device_map: "auto"
max_new_tokens: 512

# Question generation
q_device_map: "auto"
q_max_length: 500

# Response generation
timeout: 10.
skip_prompt: True
skip_special_tokens: True
r_device_map: "auto"
# r_max_length: token_max - 110 - max_new_tokens - q_max_length
r_max_length: 2000
return_source_documents: True

# Citing sources
width: 110

